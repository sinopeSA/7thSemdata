### Hill climbing ###
from typing import List, Callable, Tuple
import numpy as np
import matplotlib.pyplot as plt
from numpy.random import seed

def hill_climbing(
    objective: Callable,
    bounds: Tuple[float, float],
    iterations: int,
    step_size: float
) -> Tuple[np.ndarray, np.ndarray, List]:
    # initial point
    solution = np.random.uniform(*bounds)
    # value of the initial point
    solution_eval = objective(solution)
    # variable to keep track of solutions
    sols = []
    # running the hill climb algorithm
    for i in range(iterations):
        # getting a random candidate point
        candidate = solution + np.random.randn() * step_size
        # getting the value of the candidate using the objective function
        candidate_eval = objective(candidate)
        # if candidate is better
        if candidate_eval <= solution_eval:
            # setting solution as candidate
            solution, solution_eval = candidate, candidate_eval
            # appending solution value to sols
            sols.append(solution)
            # report progress
            print(
                f'iteration %{len(str(iterations))}d: f(%.5f) = %.5f' 
                % (i, solution, solution_eval))
    return solution, solution_eval, sols

def plot_results(objective, bounds, sols, figsize=(8,5)):
    x = np.linspace(*bounds, 100)
    y = objective(x)

    plt.figure(figsize=figsize)
    plt.plot(x, y,label="actual")
    plt.plot(sols, [objective(x) for x in sols], 'o',label="hill climbing")
    plt.legend()
    plt.grid()
    plt.show()


objective = lambda x: (x**2.0) // change here
bounds = [-5.0, 5.0] // change here
iterations = 1000
step_size = 0.1
seed(5)
best, value, sols = hill_climbing(objective, bounds, iterations, step_size)
print('The best solution is: f(%s) = %f' % (best, value))
plot_results(objective, bounds, sols)


**ALPHA BETA**

# Initial values of Aplha and Beta
MAX, MIN = 1000, -1000

# Returns optimal value for current player
# (Initially called for root and maximizer)
def minimax(depth, nodeIndex, maximizingPlayer, values, alpha, beta):
    # Terminating condition. i.e
    # leaf node is reached
    if depth == 3:
        return values[nodeIndex]

    if maximizingPlayer:
        best = MIN
        # Recur for left and right children
        for i in range(0, 2):
            val = minimax(depth + 1, nodeIndex * 2 + i,
                          False, values, alpha, beta)
            best = max(best, val)
            alpha = max(alpha, best)

            # Alpha Beta Pruning
            if beta <= alpha:
                break
        return best

    else:
        best = MAX

        # Recur for left and
        # right children
        for i in range(0, 2):

            val = minimax(depth + 1, nodeIndex * 2 + i,
                          True, values, alpha, beta)
            best = min(best, val)
            beta = min(beta, best)

            # Alpha Beta Pruning
            if beta <= alpha:
                break

        return best

# Driver Code
if __name__ == "__main__":
    values = [10, 14, -4, 13, 8, 9, 7, -11]
    print("The optimal value is :", minimax(0, 0, True, values, MIN, MAX))




**DFS**

graph = {'a':['b','c'], 'b': ['d','e'],'c':['f'], 'd':[], 'e': ['f'], 'f':[]}

visited = set()

def dfs(visited,graph,node):
    if node not in visited:
        print (node,end=" ")
        visited.add(node)
        for neighbour in graph [node]:
            dfs(visited, graph, neighbour)

dfs(visited,graph,'a')

**BFS**

graph = {'a': ['b', 'c'], 'b': ['d', 'e'], 'c': ['f'], 'd': [], 'e': ['f'], 'f': []}

visited = []
queue = []
def bfs(visited, graph, node):
    visited.append(node)
    queue.append(node)
    while queue:
        s = queue.pop(0)
        print(s, end=" ")

        for neighbour in graph[s]:
            if neighbour not in visited:
                visited.append(neighbour)
                queue.append(neighbour)

bfs(visited, graph, 'a')


### Genetic algo:
import random
import datetime

geneSet = " abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!."
target = "MSRUAS"
   
def generate_parent(length):
    genes = []
    while len(genes) < length:
        sampleSize = min(length - len(genes), len(geneSet))
        genes.extend(random.sample(geneSet, sampleSize))
    return ''.join(genes)
 
def get_fitness(guess):
    return sum(1 for expected, actual in zip(target, guess)
               if expected == actual)
 
def mutate(parent):
    index = random.randrange(0, len(parent))
    childGenes = list(parent)
    newGene, alternate = random.sample(geneSet, 2)
    childGenes[index] = alternate  \
        if newGene == childGenes[index]  \
        else newGene
    return ''.join(childGenes)
 
 
 
def display(guess):
     timeDiff = datetime.datetime.now() - startTime
     fitness = get_fitness(guess)
     print("{0}\t{1}\t{2}".format(guess, fitness, str(timeDiff)))

random.seed()
startTime = datetime.datetime.now()
bestParent = generate_parent(len(target))
bestFitness = get_fitness(bestParent)
display(bestParent)

while True:
    child = mutate(bestParent)
    childFitness = get_fitness(child)

    if bestFitness >= childFitness:
        continue
    display(child)
    if childFitness >= len(bestParent):
        break
    bestFitness = childFitness
    bestParent = child



#### fuzzy logic:
!pip install -U scikit-fuzzy

import numpy as np
import skfuzzy as fuzz
from skfuzzy import control as ctrl

quality = ctrl.Antecedent(np.arange(0, 11, 1), 'quality')
service = ctrl.Antecedent(np.arange(0, 11, 1), 'service')
tip = ctrl.Consequent(np.arange(0, 26, 1), 'tip')

quality.automf(3)
service.automf(3)

tip['low'] = fuzz.trimf(tip.universe, [0, 0, 13])
tip['medium'] = fuzz.trimf(tip.universe, [0, 13, 25])
tip['high'] = fuzz.trimf(tip.universe, [13, 25, 25])

quality['average'].view()
service.view()
tip.view()
rule1 = ctrl.Rule(quality['poor'] | service['poor'], tip['low'])
rule2 = ctrl.Rule(service['average'], tip['medium'])
rule3 = ctrl.Rule(service['good'] | quality['good'], tip['high'])

rule1.view()


tipping_ctrl = ctrl.ControlSystem([rule1, rule2, rule3])


tipping = ctrl.ControlSystemSimulation(tipping_ctrl)
tipping.input['quality'] = 6.5
tipping.input['service'] = 9.8

tipping.compute()
print (tipping.output['tip'])
tip.view(sim=tipping)



#### ANN:
import numpy as np

class NeuralNetwork():
    
    def __init__(self):
        # seeding for random number generation
        np.random.seed(1)
        
        #converting weights to a 3 by 1 matrix with values from -1 to 1 and mean of 0
        self.synaptic_weights = 2 * np.random.random((3, 1)) - 1

    def sigmoid(self, x):
        #applying the sigmoid function
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        #computing derivative to the Sigmoid function
        return x * (1 - x)

    def train(self, training_inputs, training_outputs, training_iterations):
        
        #training the model to make accurate predictions while adjusting weights continually
        for iteration in range(training_iterations):
            #siphon the training data via  the neuron
            output = self.think(training_inputs)

            #computing error rate for back-propagation
            error = training_outputs - output
            
            #performing weight adjustments
            adjustments = np.dot(training_inputs.T, error * self.sigmoid_derivative(output))

            self.synaptic_weights += adjustments

    def think(self, inputs):
        #passing the inputs via the neuron to get output   
        #converting values to floats
        
        inputs = inputs.astype(float)
        output = self.sigmoid(np.dot(inputs, self.synaptic_weights))
        return output
if __name__ == "__main__":
    #initializing the neuron class
    neural_network = NeuralNetwork()
    print("Beginning Randomly Generated Weights: ")
    print(neural_network.synaptic_weights)
    #training data consisting of 4 examples--3 input values and 1 output
    training_inputs = np.array([[0,0,1],
                                [1,1,1],
                                [1,0,1],
                                [0,1,1]])

    training_outputs = np.array([[0,1,1,0]]).T

    #training taking place
    neural_network.train(training_inputs, training_outputs, 15000)

    print("Ending Weights After Training: ")
    print(neural_network.synaptic_weights)

    user_input_one = str(input("User Input One: "))
    user_input_two = str(input("User Input Two: "))
    user_input_three = str(input("User Input Three: "))
    
    print("Considering New Situation: ", user_input_one, user_input_two, user_input_three)
    print("New Output data: ")
    print(neural_network.think(np.array([user_input_one, user_input_two, user_input_three])))




#### Genetic P2:
import random

def generate_population(size, x_boundaries, y_boundaries):
    lower_x_boundary, upper_x_boundary = x_boundaries
    lower_y_boundary, upper_y_boundary = y_boundaries

    population = []
    for i in range(size):
        individual = {
            "x": random.uniform(lower_x_boundary, upper_x_boundary),
            "y": random.uniform(lower_y_boundary, upper_y_boundary),
        }
        population.append(individual)

    return population
import math

def apply_function(individual):
    x = individual["x"]
    y = individual["y"]
    return math.sin(math.sqrt(x ** 2 + y ** 2))
generations = 100

population = generate_population(size=10, x_boundaries=(-4, 4), y_boundaries=(-4, 4))

i = 1
while True:
    print(f"ðŸ§¬ GENERATION {i}")

    for individual in population:
        print(individual)

    if i == generations:
        break

    i += 1

    # Make next generation...
def choice_by_roulette(sorted_population, fitness_sum):
    offset = 0
    normalized_fitness_sum = fitness_sum

    lowest_fitness = apply_function(sorted_population[0])
    if lowest_fitness < 0:
        offset = -lowest_fitness
        normalized_fitness_sum += offset * len(sorted_population)

    draw = random.uniform(0, 1)

    accumulated = 0
    for individual in sorted_population:
        fitness = apply_function(individual) + offset
        probability = fitness / normalized_fitness_sum
        accumulated += probability

        if draw <= accumulated:
            return individual
def sort_population_by_fitness(population):
    return sorted(population, key=apply_function)


def crossover(individual_a, individual_b):
    xa = individual_a["x"]
    ya = individual_a["y"]

    xb = individual_b["x"]
    yb = individual_b["y"]

    return {"x": (xa + xb) / 2, "y": (ya + yb) / 2}


def mutate(individual):
    next_x = individual["x"] + random.uniform(-0.05, 0.05)
    next_y = individual["y"] + random.uniform(-0.05, 0.05)

    lower_boundary, upper_boundary = (-4, 4)

    # Guarantee we keep inside boundaries
    next_x = min(max(next_x, lower_boundary), upper_boundary)
    next_y = min(max(next_y, lower_boundary), upper_boundary)

    return {"x": next_x, "y": next_y}


def make_next_generation(previous_population):
    next_generation = []
    sorted_by_fitness_population = sort_population_by_fitness(previous_population)
    population_size = len(previous_population)
    fitness_sum = sum(apply_function(individual) for individual in population)

    for i in range(population_size):
        first_choice = choice_by_roulette(sorted_by_fitness_population, fitness_sum)
        second_choice = choice_by_roulette(sorted_by_fitness_population, fitness_sum)

        individual = crossover(first_choice, second_choice)
        individual = mutate(individual)
        next_generation.append(individual)

    return next_generation


generations = 100

population = generate_population(size=10, x_boundaries=(-4, 4), y_boundaries=(-4, 4))

i = 1
while True:
    print(f"ðŸ§¬ GENERATION {i}")

    for individual in population:
        print(individual, apply_function(individual))

    if i == generations:
        break

    i += 1

    population = make_next_generation(population)

best_individual = sort_population_by_fitness(population)[-1]
print("\nðŸ”¬ FINAL RESULT")
print(best_individual, apply_function(best_individual))



#### hill 2:
import random

def randomSolution(tsp):
  cities = list(range(len(tsp)))
  solution = []
  for i in range(len(tsp)):
    randomCity = cities[random.randint(0, len(cities) - 1)]
    solution.append(randomCity)
    cities.remove(randomCity)
  return solution

def routeLength(tsp, solution):
  routeLength = 0
  for i in range(len(solution)):
    routeLength += tsp[solution[i - 1]][solution[i]]
  return routeLength

def getNeighbours(solution):
  neighbours = []
  for i in range(len(solution)):
    for j in range(i + 1, len(solution)):
      neighbour = solution.copy()
      neighbour[i] = solution[j]
      neighbour[j] = solution[i]
      neighbours.append(neighbour)
  return neighbours

def getBestNeighbour(tsp, neighbours):
  bestRouteLength = routeLength(tsp, neighbours[0])
  bestNeighbour = neighbours[0]
  for neighbour in neighbours:
    currentRouteLength = routeLength(tsp, neighbour)
    if currentRouteLength < bestRouteLength:
      bestRouteLength = currentRouteLength
      bestNeighbour = neighbour
  return bestNeighbour, bestRouteLength

def hillClimbing(tsp):
  currentSolution = randomSolution(tsp)
  currentRouteLength = routeLength(tsp, currentSolution)
  neighbours = getNeighbours(currentSolution)
  bestNeighbour, bestNeighbourRouteLength = getBestNeighbour(tsp, neighbours)
  
  while bestNeighbourRouteLength < currentRouteLength:
    currentSolution = bestNeighbour
    currentRouteLength = bestNeighbourRouteLength
    neighbours = getNeighbours(currentSolution)
    bestNeighbour, bestNeighbourRouteLength = getBestNeighbour(tsp, neighbours)
  return currentSolution, currentRouteLength

def main():
  tsp = [
         [0, 400, 600, 300],
         [400, 0, 300, 500],
         [600, 300, 0, 200],
         [300, 500, 200, 0]
         ]
  print(hillClimbing(tsp))

if __name__ == "__main__":
  main()



